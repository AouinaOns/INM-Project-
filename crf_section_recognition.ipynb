{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crf_section_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eaaIWf02GmDl",
        "yJ_cdSnqXa-O"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaaIWf02GmDl",
        "colab_type": "text"
      },
      "source": [
        "## execute this cell first\n",
        "class to retrieve the sentences from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c08nyusMmdZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A class to retrieve the sentences from the dataset\n",
        "class getsentence(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1.0\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"sentence\"].values.tolist(),\n",
        "                                                           s[\"tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"pmid\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "\n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"pmid : {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4x7cN0MZ_Hw",
        "colab_type": "text"
      },
      "source": [
        "## read the data\n",
        "Note : to run the CRf the training data must be annotated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyAzrxpmZkmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annotated = pd.read_csv('all_annotated.csv')\n",
        "data = annotated.dropna()\n",
        "getter = getsentence(data)\n",
        "sentences = getter.sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ_cdSnqXa-O",
        "colab_type": "text"
      },
      "source": [
        "## utils functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty1dm3EiGkp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentense_number(abstract, token):\n",
        "  blob = TextBlob(abstract)\n",
        "  sous_sentenses = []\n",
        "  for a in blob.sentences:\n",
        "    sous_sentenses.append(str(a))\n",
        "\n",
        "  num = sous_sentenses.index(token)+1\n",
        "  return num\n",
        "\n",
        "def sentence_position(text,token):\n",
        "  indice = 0\n",
        "  joker = 0\n",
        "  lsx = text.split()\n",
        "  chunk_size = int(len(lsx)/10)\n",
        "  #print(chunk_size)\n",
        "  output = [lsx[i:i+chunk_size] for i in range(0, len(lsx), chunk_size)]\n",
        "  liste_des_parties = []\n",
        "  for ab in output:\n",
        "    part = ''\n",
        "    for text in ab:\n",
        "      part = part + ' ' + text\n",
        "    liste_des_parties.append(part.lstrip())\n",
        "\n",
        "  #print(liste_des_parties)\n",
        "  first3 = ''\n",
        "  last3 = ''\n",
        "  for ele in token.split()[:3]:\n",
        "    first3 = first3 + ' ' + ele\n",
        "  for ele in token.split()[-3:]:\n",
        "    last3 = last3 + ' ' + ele\n",
        "  \n",
        "  token_head = first3.lstrip()\n",
        "  token_tail = last3.lstrip()\n",
        "  #print(token_head)\n",
        "  #print(token_tail)\n",
        "  #retrunn the index of the part that contains the sentence, If after cutting a sentence straddles 2 sections, we take the max index.\n",
        "  for part in liste_des_parties:\n",
        "    if token in part:\n",
        "      indice = liste_des_parties.index(part)+1\n",
        "    elif token_tail in part:\n",
        "      indice = liste_des_parties.index(part)+1\n",
        "    elif token_head in part:\n",
        "      joker = liste_des_parties.index(part)+1\n",
        "      \n",
        "\n",
        "  if indice == 0:\n",
        "    indice = joker\n",
        "  return indice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--zaD1sIfp0l",
        "colab_type": "text"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSBjAj1rgRpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Ilist = ['intervention','exercise','increase','life','quality','physical','examine','health','reduce','aim','program','care','effectiveness',\n",
        "'interventions','activity','fatigue','group','dietary','information', 'distress']\n",
        "Mlist = ['exercise','health','outcome','physical','pain','fatigue','diet','anxiety','depression','experimental',\n",
        "'body','hospital','medical','participate','blood','model','dietary','practice','vitamin', 'symptoms']\n",
        "Rlist = ['group','intervention','increase','compare','effect','score','participants','change','baseline','difference','show',\n",
        "'follow','time','mean','decrease','level','lower','higher','find','exercise']\n",
        "Clist = ['health','suggest','benefit','decrease','supplementation','future','research','nurse','enhance','feasible',\n",
        "'dietary','appear','distress','potential','diet','psychological','weight','beneficial','aerobic','approach']\n",
        "\n",
        "\n",
        "def get_token_voca(token):\n",
        "  i = sum(el in token.split() for el in Ilist)\n",
        "  m = sum(el in token.split() for el in Mlist)\n",
        "  r = sum(el in token.split() for el in Rlist)\n",
        "  c = sum(el in token.split() for el in Clist)\n",
        "  if i>m and i>r and i>c:\n",
        "    return Ilist\n",
        "  elif m>i and m>r and m>c:\n",
        "    return Mlist\n",
        "  elif r>i and r>m and r>c:\n",
        "    return Rlist\n",
        "  elif c>i and c>m and c>r:\n",
        "    return Clist\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "pLength, pRelLength = 0,0\n",
        "nLength, nRelLength = 0,0\n",
        "#les premiers mots de la phrase précédente (p0, p1)\n",
        "#la longueur absolue de phrase précédente (pLength)\n",
        "#la longueur relative de phrase précédente (pRelLength)\n",
        "#la longueur absolue de phrase suivante (nLength)\n",
        "#la longueur relative de phrase suivante (nRelLength)\n",
        "\n",
        "\n",
        "def sentencefeatures(sentences,i):\n",
        "  \n",
        "  token = sentences[i][0].lstrip()\n",
        "\n",
        "  features = {\n",
        "      'token': token,\n",
        "      'first_3_words': token.split()[:3],\n",
        "      'absLength': len(token.split()),\n",
        "      'lexique' : get_token_voca(token), \n",
        "      #'relLength': len(token.split())/len(abstract.split()),\n",
        "      #'absNum' : sentense_number(abstract, token),\n",
        "      #'relNum' : sentence_position(abstract,token)            \n",
        "              \n",
        "  }\n",
        "  \n",
        "  if i > 0:\n",
        "    previous_token = sentences[i-1][0].lstrip()\n",
        "    features.update({\n",
        "        'firstP_2_words': previous_token.split()[:2],\n",
        "        'pLength': len(previous_token.split()),\n",
        "        #'pRelLength': len(previous_token.split())/len(abstract.split())\n",
        "        })\n",
        "  \n",
        "  if i < len(sentences)-1:\n",
        "    next_token = sentences[i+1][0].lstrip()\n",
        "    features.update({\n",
        "        'firstN_2_words': next_token.split()[:2],\n",
        "        'nLength': len(next_token.split()),\n",
        "        #'nRelLength': len(next_token.split())/len(abstract.split())\n",
        "        })\n",
        "\n",
        "  return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [sentencefeatures(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [tag for sentence,tag in sent]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Nohu-Is3LE",
        "colab_type": "text"
      },
      "source": [
        "## Training CRF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRpvyxeLtb0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install sklearn_crfsuite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGfeVa13tfMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install eli5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU7p7tegtgzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install python-crfsuite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRvk4a_blYHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn_crfsuite import CRF, scorers, metrics\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "from sklearn.metrics import classification_report, make_scorer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwOkGyaps0sJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = [sent2features(phrase) for phrase in sentences]\n",
        "y = [sent2labels(phrase) for phrase in sentences] #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Pc_s-stkas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "!!! Note :  if there is an error about fitting the data , please check the output of the function get_token_voca(token),\n",
        "specially this feature (  lexique' : get_token_voca(token) ) has a bad data input like None or NoType..'\n",
        "\"\"\"\n",
        "import sklearn_crfsuite\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',  # options: 'l2sgd', 'lbfgs', \n",
        "    c1=0.0418,           # 0.015, # not applicable for 'l2sgd'\n",
        "    c2=0.00056,          # 0.0037,\n",
        "    max_iterations=100, #100,\n",
        "    all_possible_transitions=True,\n",
        "    verbose=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-soD7HWLtpIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = cross_val_predict(estimator=crf, X=X_test, y=y_test, cv=5)\n",
        "report = flat_classification_report(y_pred=pred, y_true=y_test)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6iiuSFPpmJN",
        "colab_type": "text"
      },
      "source": [
        "## Visualisation\n",
        "display the most important functionalities for your model (here we have chosen 30), also the transitions between the sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85Uiq3bZtsAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import eli5\n",
        "eli5.show_weights(crf, top=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R468TshUqXrr",
        "colab_type": "text"
      },
      "source": [
        "### - we show here for each functionality its importance in relation to all the sections.\n",
        "### - to view all the features, please put the first word of each feature in the variable \"**feature_re**\" and **execute** as long as the number of features in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXilrlc5ttl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eli5.show_weights(crf, top=10, feature_re='^l',\n",
        "                  horizontal_layout=False, show=['targets'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XVrYKslrt_A",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    }
  ]
}