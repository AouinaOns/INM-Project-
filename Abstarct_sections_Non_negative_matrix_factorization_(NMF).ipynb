{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Abstarct_sections_Non_negative_matrix_factorization_(NMF).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OdkQC5jZfu2z",
        "fH1TQnjMf3Qk",
        "AbatLmZ0gAzK",
        "2pI7TDN0hfgi",
        "ozOibhdrhWnS",
        "JbXzZCaBj-0T"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9nugWkOlalN",
        "colab_type": "text"
      },
      "source": [
        "## Reference\n",
        "Hi, you can find the article reference on this site: https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdkQC5jZfu2z",
        "colab_type": "text"
      },
      "source": [
        "## Importation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tOzRYW6dvlx",
        "colab_type": "code",
        "outputId": "f82a61e6-0036-47ed-ecdd-0ca378ae2fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import sklearn\n",
        "import sys\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from gensim.models import ldamodel\n",
        "import gensim.corpora\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.preprocessing import normalize\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "all_stopwords = stopwords.words('english')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH1TQnjMf3Qk",
        "colab_type": "text"
      },
      "source": [
        "## Functions + stop_words_list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Yt_qf-fbcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords_other = ['middle', 'aged', 'author', 'followup', 'improve', 'conclusion', 'day', 'also', 'men', \n",
        "                   'background', 'patient', 'group', 'cancer', 'patients', 'groups', 'study', 'treatment', \n",
        "                   'level', 'show', 'control', 'significant', 'effects', 'effect', 'trial', 'year', 'respectively',\n",
        "                   'data', 'risk', 'compare', 'clinical', 'breast', 'breasts', 'include', 'receive', 'use', 'response', \n",
        "                   'two', 'cycle', 'month', 'disease', 'week', 'women', 'one', 'rate', 'three', 'combination', 'total',\n",
        "                   'follow', 'low', 'increase', 'among', 'result', 'incidence', 'lung', 'population', 'screening', \n",
        "                   'screen', 'high', 'tumor', 'case', 'stage', 'present', 'may', 'time', 'find', 'local', 'however',\n",
        "                   'evaluate', 'four', 'course', 'hight', 'give', 'test', 'age', 'subject', 'report', 'identify', \n",
        "                   'need', 'outcome', 'provide', 'information', 'support', 'well', 'design', 'assess', 'participant',\n",
        "                   'child', 'cell', 'human', 'drug', 'suggest', 'induce', 'demonstrate', 'change', 'observe', 'decrease',\n",
        "                   'developement', 'associate', 'expression', 'survival', 'primary', 'recieve', 'median', 'treat',\n",
        "                   'difference', 'administration', 'overall', 'six', 'first','efficacy', 'administer', 'five', 'occur',\n",
        "                   'without', 'every', 'range', 'objective', 'grade', 'evaluable', 'versus', 'cause', 'factor', 'type',\n",
        "                   'different', 'specific', 'various', 'several', 'new', 'function', 'normal', 'although', 'involve', \n",
        "                   'cloud', 'association', 'individual', 'estimate', 'baseline', 'analysis', 'significantly', 'great',\n",
        "                   'relationship', 'symptom', 'research', 'impact', 'problem', 'regard', 'knowledge', 'effective', \n",
        "                   'evidence', 'condition', 'quality', 'phase', 'agent', 'determine', 'develop', 'mean', 'see', \n",
        "                   'recurrence', 'procedure', 'operation', 'number', 'cost', 'chemotherapy', 'pain', 'toxicity',\n",
        "                   'therapy', 'surgery', 'reccurence', 'radiotherapy', 'complete', 'schedule', 'phase_ii', 'could',\n",
        "                   'therapeutic', 'lead', 'gene', 'mortatlity', 'risk_factor', 'death', 'mammography', 'experience',\n",
        "                   'measure', 'scale', 'questionnaire', 'relate', 'concern', 'reccurrence', 'complication',\n",
        "                   'management', 'program', 'protocol', 'plan', 'mortality', 'similar', 'alone', 'either', 'enrol',\n",
        "                   'concentration', 'tissue', 'base', 'describe', 'role', 'system', 'discuss', 'process', 'focus', \n",
        "                   'consider', 'cervical', 'score', 'positive', 'average', 'randomized', 'within', 'site', 'image',\n",
        "                   'obtain', 'area', 'to' , 'the','this',  'non' ,'exposure', 'supplement', 'old', 'whether', '12',\n",
        "                   '50', '30', 'mg','the', 'fifty', 'ISRCTN', 'Gov' , 'gov', 'clinicaltrials', 'trial' , 'UTN', 'all',\n",
        "                   'cm', 'these', 'trials', 'interventions' , 'results','rat', 'use','long', 'nsclc', 'undergo','CM','as', 'AS','ALL',\n",
        "                   'effectiveness','all', 'use','months', 'ci', 'ub', 'lt', 'vs', 'il', 'gt', 'up', 'weeks', 'follow']\n",
        "        \n",
        "all_stopwords.extend(stopwords_other)\n",
        "\n",
        "exclude =  '!\"\"#$1234567890%&\\'()*,+/:;<=>?@[\\\\]^_`{|}~\"'''\n",
        "def remove_punctuation(x):\n",
        "    \"\"\"\n",
        "    Helper function to remove punctuation from a string\n",
        "    x: any string\n",
        "    \"\"\"\n",
        "    try:\n",
        "        x = ''.join(ch for ch in x if ch not in exclude)\n",
        "    except:\n",
        "        pass\n",
        "    return x\n",
        "\n",
        "token=ToktokTokenizer()\n",
        "lemma=WordNetLemmatizer()\n",
        "\n",
        "def lemitizeWords(text):\n",
        "    words=token.tokenize(text)\n",
        "    listLemma=[]\n",
        "    for w in words:\n",
        "        x=lemma.lemmatize(w, pos=\"v\")\n",
        "        listLemma.append(x)\n",
        "    return ' '.join(map(str, listLemma))\n",
        "\n",
        "def removeStopWords(text):\n",
        "    words = token.tokenize(text)\n",
        "    filtered = [w for w in words if not w in all_stopwords]\n",
        "    return ' '.join(map(str, filtered))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbatLmZ0gAzK",
        "colab_type": "text"
      },
      "source": [
        "## Data reading\n",
        "the data file below is the file that contains the four cut sections of an abstract."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwK0sHXehSe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read file \n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"abstract_cuted .csv\", sep = ',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pI7TDN0hfgi",
        "colab_type": "text"
      },
      "source": [
        "## NMF for the \"introduction\" section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtCHCf2FhrRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data pre-prcessing\n",
        "data['introduction'] = data['introduction'].apply(lambda x: remove_punctuation(x))\n",
        "data['introduction'] = data['introduction'].str.lower()\n",
        "data['introduction'] = data['introduction'].apply(lambda x: lemitizeWords(x))\n",
        "data['introduction'] = data['introduction'].apply(lambda x: removeStopWords(x))\n",
        "\n",
        "data_text = data[['introduction']]\n",
        "data_text = data_text.astype('str')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdnIf0lIjMRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# recode the task needed to properly clean the data again => be sure there is no stop_words yet. \n",
        "for idx in range(len(data_text)):\n",
        "    #go through each word in each data_text row, remove stopwords, and set them on the index.\n",
        "    data_text.iloc[idx]['introduction'] = [word for word in data_text.iloc[idx]['introduction'].split(' ') if word not in all_stopwords]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LApRJ0djQr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_headlines = [value[0] for value in data_text.iloc[0:].values]\n",
        "num_topics = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqw0MFMGjSjq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4549695f-ae4a-46a4-e16a-3a1b8d628af3"
      },
      "source": [
        "# get the (matrix , vectors) for the model input \n",
        "train_headlines_sentences = [' '.join(text) for text in train_headlines]\n",
        "vectorizer = CountVectorizer(analyzer='word', max_features=5000)\n",
        "x_counts = vectorizer.fit_transform(train_headlines_sentences)\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "x_tfidf = transformer.fit_transform(x_counts)\n",
        "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n",
        "#obtain a NMF model.\n",
        "model = NMF(n_components=num_topics, init='nndsvd')\n",
        "#fit the model\n",
        "model.fit(xtfidf_norm)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
              "    n_components=4, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a_NnHn8jXt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_nmf_topics(model, n_top_words):\n",
        "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
        "    feat_names = vectorizer.get_feature_names()\n",
        "    word_dict = {};\n",
        "    for i in range(num_topics):\n",
        "        \n",
        "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
        "        words_ids = model.components_[i].argsort()[:-n_top_words - 1:-1]\n",
        "        words = [feat_names[key] for key in words_ids]\n",
        "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words\n",
        "    \n",
        "    return pd.DataFrame(word_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56vekwIyjwrU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "outputId": "be097762-c2e7-4c43-a347-58b1dba3f765"
      },
      "source": [
        "print(\"------------------- NMF_introduction -----------------------\")\n",
        "get_nmf_topics(model, 25)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------- NMF_introduction -----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic # 01</th>\n",
              "      <th>Topic # 02</th>\n",
              "      <th>Topic # 03</th>\n",
              "      <th>Topic # 04</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>exercise</td>\n",
              "      <td>nurse</td>\n",
              "      <td>intervention</td>\n",
              "      <td>psychotherapeutic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fatigue</td>\n",
              "      <td>older</td>\n",
              "      <td>immune</td>\n",
              "      <td>gastrointestinal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>physical</td>\n",
              "      <td>breathlessness</td>\n",
              "      <td>prostate</td>\n",
              "      <td>acupuncture</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>survivors</td>\n",
              "      <td>ethnic</td>\n",
              "      <td>based</td>\n",
              "      <td>postoperative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>life</td>\n",
              "      <td>outcomes</td>\n",
              "      <td>care</td>\n",
              "      <td>restoration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>train</td>\n",
              "      <td>education</td>\n",
              "      <td>health</td>\n",
              "      <td>gastric</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>examine</td>\n",
              "      <td>intervention</td>\n",
              "      <td>psychological</td>\n",
              "      <td>combine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>related</td>\n",
              "      <td>led</td>\n",
              "      <td>reduce</td>\n",
              "      <td>medication</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>activity</td>\n",
              "      <td>examine</td>\n",
              "      <td>randomize</td>\n",
              "      <td>colorectal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>qol</td>\n",
              "      <td>anxiety</td>\n",
              "      <td>psychosocial</td>\n",
              "      <td>investigate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>aerobic</td>\n",
              "      <td>practice</td>\n",
              "      <td>cognitive</td>\n",
              "      <td>recent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>investigate</td>\n",
              "      <td>apns</td>\n",
              "      <td>distress</td>\n",
              "      <td>points</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>cancer</td>\n",
              "      <td>homecare</td>\n",
              "      <td>colorectal</td>\n",
              "      <td>ea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>aim</td>\n",
              "      <td>diagnose</td>\n",
              "      <td>feasibility</td>\n",
              "      <td>electroacupuncture</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>purpose</td>\n",
              "      <td>analyze</td>\n",
              "      <td>self</td>\n",
              "      <td>time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>dose</td>\n",
              "      <td>compliance</td>\n",
              "      <td>examine</td>\n",
              "      <td>cc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>common</td>\n",
              "      <td>caregivers</td>\n",
              "      <td>aim</td>\n",
              "      <td>flash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>radiation</td>\n",
              "      <td>supportive</td>\n",
              "      <td>decision</td>\n",
              "      <td>mutant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>yoga</td>\n",
              "      <td>care</td>\n",
              "      <td>counsel</td>\n",
              "      <td>pras</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>unknown</td>\n",
              "      <td>conventional</td>\n",
              "      <td>dietary</td>\n",
              "      <td>neoplasm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>fitness</td>\n",
              "      <td>utilization</td>\n",
              "      <td>life</td>\n",
              "      <td>postoperation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>adjuvant</td>\n",
              "      <td>depression</td>\n",
              "      <td>behavioral</td>\n",
              "      <td>hot</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>outcomes</td>\n",
              "      <td>newly</td>\n",
              "      <td>promote</td>\n",
              "      <td>gi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>crf</td>\n",
              "      <td>practitioners</td>\n",
              "      <td>early</td>\n",
              "      <td>gum</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>health</td>\n",
              "      <td>service</td>\n",
              "      <td>community</td>\n",
              "      <td>capsule</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Topic # 01      Topic # 02     Topic # 03          Topic # 04\n",
              "0      exercise           nurse   intervention   psychotherapeutic\n",
              "1       fatigue           older         immune    gastrointestinal\n",
              "2      physical  breathlessness       prostate         acupuncture\n",
              "3     survivors          ethnic          based       postoperative\n",
              "4          life        outcomes           care         restoration\n",
              "5         train       education         health             gastric\n",
              "6       examine    intervention  psychological             combine\n",
              "7       related             led         reduce          medication\n",
              "8      activity         examine      randomize          colorectal\n",
              "9           qol         anxiety   psychosocial         investigate\n",
              "10      aerobic        practice      cognitive              recent\n",
              "11  investigate            apns       distress              points\n",
              "12       cancer        homecare     colorectal                  ea\n",
              "13          aim        diagnose    feasibility  electroacupuncture\n",
              "14      purpose         analyze           self                time\n",
              "15         dose      compliance        examine                  cc\n",
              "16       common      caregivers            aim               flash\n",
              "17    radiation      supportive       decision              mutant\n",
              "18         yoga            care        counsel                pras\n",
              "19      unknown    conventional        dietary            neoplasm\n",
              "20      fitness     utilization           life       postoperation\n",
              "21     adjuvant      depression     behavioral                 hot\n",
              "22     outcomes           newly        promote                  gi\n",
              "23          crf   practitioners          early                 gum\n",
              "24       health         service      community             capsule"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozOibhdrhWnS",
        "colab_type": "text"
      },
      "source": [
        "## NMF for the \"results\" section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggc8YowfeRt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data pre-prcessing\n",
        "data['resultats'] = data['resultats'].apply(lambda x: remove_punctuation(x))\n",
        "data['resultats'] = data['resultats'].str.lower()\n",
        "data['resultats'] = data['resultats'].apply(lambda x: lemitizeWords(x))\n",
        "data['resultats'] = data['resultats'].apply(lambda x: removeStopWords(x))\n",
        "\n",
        "data_text = data[['resultats']]\n",
        "data_text = data_text.astype('str')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hToXIKxhi9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# recode the task needed to properly clean the data again => be sure there is no stop_words yet. \n",
        "for idx in range(len(data_text)):\n",
        "    #go through each word in each data_text row, remove stopwords, and set them on the index.\n",
        "    data_text.iloc[idx]['resultats'] = [word for word in data_text.iloc[idx]['resultats'].split(' ') if word not in all_stopwords]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7nyhM1jiRx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_headlines = [value[0] for value in data_text.iloc[0:].values]\n",
        "num_topics = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAQccR8xiZpZ",
        "colab_type": "code",
        "outputId": "3932a037-73ef-48b9-9463-4cd80de5a42a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# get the (matrix , vectors) for the model input \n",
        "train_headlines_sentences = [' '.join(text) for text in train_headlines]\n",
        "vectorizer = CountVectorizer(analyzer='word', max_features=5000)\n",
        "x_counts = vectorizer.fit_transform(train_headlines_sentences)\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "x_tfidf = transformer.fit_transform(x_counts)\n",
        "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n",
        "#obtain a NMF model.\n",
        "model = NMF(n_components=num_topics, init='nndsvd')\n",
        "#fit the model\n",
        "model.fit(xtfidf_norm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
              "    n_components=4, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBwL6dxXitt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_nmf_topics(model, n_top_words):\n",
        "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
        "    feat_names = vectorizer.get_feature_names()\n",
        "    word_dict = {};\n",
        "    for i in range(num_topics):\n",
        "        \n",
        "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
        "        words_ids = model.components_[i].argsort()[:-n_top_words - 1:-1]\n",
        "        words = [feat_names[key] for key in words_ids]\n",
        "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words\n",
        "    \n",
        "    return pd.DataFrame(word_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RzS-iqSiy-q",
        "colab_type": "code",
        "outputId": "c514321e-b88e-4bbe-cdc0-708f4989d111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        }
      },
      "source": [
        "print(\"------------------- NMF_resultats -----------------------\")\n",
        "get_nmf_topics(model, 30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------- NMF_resultats -----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic # 01</th>\n",
              "      <th>Topic # 02</th>\n",
              "      <th>Topic # 03</th>\n",
              "      <th>Topic # 04</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>intervention</td>\n",
              "      <td>cure</td>\n",
              "      <td>anxiety</td>\n",
              "      <td>exercise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>participants</td>\n",
              "      <td>metoclopramide</td>\n",
              "      <td>plt</td>\n",
              "      <td>fatigue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>up</td>\n",
              "      <td>drainage</td>\n",
              "      <td>experimental</td>\n",
              "      <td>greater</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>years</td>\n",
              "      <td>gastric</td>\n",
              "      <td>depression</td>\n",
              "      <td>physical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>follow</td>\n",
              "      <td>acupuncture</td>\n",
              "      <td>lower</td>\n",
              "      <td>puu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>arm</td>\n",
              "      <td>volume</td>\n",
              "      <td>group</td>\n",
              "      <td>severity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>second</td>\n",
              "      <td>plt</td>\n",
              "      <td>life</td>\n",
              "      <td>capacity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>care</td>\n",
              "      <td>respectively</td>\n",
              "      <td>higher</td>\n",
              "      <td>body</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>older</td>\n",
              "      <td>reduce</td>\n",
              "      <td>reduce</td>\n",
              "      <td>puaua</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>pap</td>\n",
              "      <td>differences</td>\n",
              "      <td>symptoms</td>\n",
              "      <td>aerobic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>greater</td>\n",
              "      <td>regression</td>\n",
              "      <td>differences</td>\n",
              "      <td>sleep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>crc</td>\n",
              "      <td>atrophy</td>\n",
              "      <td>improvement</td>\n",
              "      <td>pualtua</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>smear</td>\n",
              "      <td>metaplasia</td>\n",
              "      <td>statistically</td>\n",
              "      <td>differences</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>higher</td>\n",
              "      <td>relative</td>\n",
              "      <td>state</td>\n",
              "      <td>functional</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>month</td>\n",
              "      <td>granules</td>\n",
              "      <td>stress</td>\n",
              "      <td>ua</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>group</td>\n",
              "      <td>intestinal</td>\n",
              "      <td>comparison</td>\n",
              "      <td>postoperative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>differences</td>\n",
              "      <td>pylori</td>\n",
              "      <td>better</td>\n",
              "      <td>less</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>proportion</td>\n",
              "      <td>tablets</td>\n",
              "      <td>reduction</td>\n",
              "      <td>walk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>standard</td>\n",
              "      <td>sham</td>\n",
              "      <td>cd</td>\n",
              "      <td>days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>per</td>\n",
              "      <td>rates</td>\n",
              "      <td>self</td>\n",
              "      <td>activity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>usual</td>\n",
              "      <td>ascorbic</td>\n",
              "      <td>acupuncture</td>\n",
              "      <td>fat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>post</td>\n",
              "      <td>anti</td>\n",
              "      <td>health</td>\n",
              "      <td>strength</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>fat</td>\n",
              "      <td>produce</td>\n",
              "      <td>distress</td>\n",
              "      <td>distance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>satisfaction</td>\n",
              "      <td>infection</td>\n",
              "      <td>physical</td>\n",
              "      <td>qol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>postoperatively</td>\n",
              "      <td>basic</td>\n",
              "      <td>related</td>\n",
              "      <td>mass</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Topic # 01      Topic # 02     Topic # 03     Topic # 04\n",
              "0      intervention            cure        anxiety       exercise\n",
              "1      participants  metoclopramide            plt        fatigue\n",
              "2                up        drainage   experimental        greater\n",
              "3             years         gastric     depression       physical\n",
              "4            follow     acupuncture          lower            puu\n",
              "5               arm          volume          group       severity\n",
              "6            second             plt           life       capacity\n",
              "7              care    respectively         higher           body\n",
              "8             older          reduce         reduce          puaua\n",
              "9               pap     differences       symptoms        aerobic\n",
              "10          greater      regression    differences          sleep\n",
              "11              crc         atrophy    improvement        pualtua\n",
              "12            smear      metaplasia  statistically    differences\n",
              "13           higher        relative          state     functional\n",
              "14            month        granules         stress             ua\n",
              "15            group      intestinal     comparison  postoperative\n",
              "16      differences          pylori         better           less\n",
              "17       proportion         tablets      reduction           walk\n",
              "18         standard            sham             cd           days\n",
              "19              per           rates           self       activity\n",
              "20            usual        ascorbic    acupuncture            fat\n",
              "21             post            anti         health       strength\n",
              "22              fat         produce       distress       distance\n",
              "23     satisfaction       infection       physical            qol\n",
              "24  postoperatively           basic        related           mass"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbXzZCaBj-0T",
        "colab_type": "text"
      },
      "source": [
        "## NMF for the \"methods\" section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DZlV0l9kHP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data pre-prcessing\n",
        "data['methods'] = data['methods'].apply(lambda x: remove_punctuation(x))\n",
        "data['methods'] = data['methods'].str.lower()\n",
        "data['methods'] = data['methods'].apply(lambda x: lemitizeWords(x))\n",
        "data['methods'] = data['methods'].apply(lambda x: removeStopWords(x))\n",
        "\n",
        "data_text = data[['methods']]\n",
        "data_text = data_text.astype('str')\n",
        "\n",
        "# recode the task needed to properly clean the data again => be sure there is no stop_words yet. \n",
        "for idx in range(len(data_text)):\n",
        "    #go through each word in each data_text row, remove stopwords, and set them on the index.\n",
        "    data_text.iloc[idx]['methods'] = [word for word in data_text.iloc[idx]['methods'].split(' ') if word not in all_stopwords]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYqzWJjFkS-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_headlines = [value[0] for value in data_text.iloc[0:].values]\n",
        "num_topics = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uofvGb8JkV8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the (matrix , vectors) for the model input \n",
        "train_headlines_sentences = [' '.join(text) for text in train_headlines]\n",
        "vectorizer = CountVectorizer(analyzer='word', max_features=5000)\n",
        "x_counts = vectorizer.fit_transform(train_headlines_sentences)\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "x_tfidf = transformer.fit_transform(x_counts)\n",
        "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n",
        "#obtain a NMF model.\n",
        "model = NMF(n_components=num_topics, init='nndsvd')\n",
        "#fit the model\n",
        "model.fit(xtfidf_norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvNx5p5DkY5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_nmf_topics(model, n_top_words):\n",
        "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
        "    feat_names = vectorizer.get_feature_names()\n",
        "    word_dict = {};\n",
        "    for i in range(num_topics):\n",
        "        \n",
        "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
        "        words_ids = model.components_[i].argsort()[:-n_top_words - 1:-1]\n",
        "        words = [feat_names[key] for key in words_ids]\n",
        "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words\n",
        "    \n",
        "    return pd.DataFrame(word_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L5vZD36ka9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"------------------- NMF_methods -----------------------\")\n",
        "get_nmf_topics(model, 30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIZlIUnvktmy",
        "colab_type": "text"
      },
      "source": [
        "## NMF for the \" conclusion\" section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22xpB8chkrOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data pre-prcessing\n",
        "data['conclusion'] = data['conclusion'].apply(lambda x: remove_punctuation(x))\n",
        "data['conclusion'] = data['conclusion'].str.lower()\n",
        "data['conclusion'] = data['conclusion'].apply(lambda x: lemitizeWords(x))\n",
        "data['conclusion'] = data['conclusion'].apply(lambda x: removeStopWords(x))\n",
        "\n",
        "data_text = data[['conclusion']]\n",
        "data_text = data_text.astype('str')\n",
        "\n",
        "# recode the task needed to properly clean the data again => be sure there is no stop_words yet. \n",
        "for idx in range(len(data_text)):\n",
        "    #go through each word in each data_text row, remove stopwords, and set them on the index.\n",
        "    data_text.iloc[idx]['conclusion'] = [word for word in data_text.iloc[idx]['conclusion'].split(' ') if word not in all_stopwords]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISNuoaV1kzcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_headlines = [value[0] for value in data_text.iloc[0:].values]\n",
        "num_topics = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMIZbFrRk2IL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "912312ab-5000-4635-a779-55739b206908"
      },
      "source": [
        "# get the (matrix , vectors) for the model input \n",
        "train_headlines_sentences = [' '.join(text) for text in train_headlines]\n",
        "vectorizer = CountVectorizer(analyzer='word', max_features=5000)\n",
        "x_counts = vectorizer.fit_transform(train_headlines_sentences)\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "x_tfidf = transformer.fit_transform(x_counts)\n",
        "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n",
        "#obtain a NMF model.\n",
        "model = NMF(n_components=num_topics, init='nndsvd')\n",
        "#fit the model\n",
        "model.fit(xtfidf_norm)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
              "    n_components=4, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ6AHh-3k5gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_nmf_topics(model, n_top_words):\n",
        "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
        "    feat_names = vectorizer.get_feature_names()\n",
        "    word_dict = {};\n",
        "    for i in range(num_topics):\n",
        "        \n",
        "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
        "        words_ids = model.components_[i].argsort()[:-n_top_words - 1:-1]\n",
        "        words = [feat_names[key] for key in words_ids]\n",
        "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words\n",
        "    \n",
        "    return pd.DataFrame(word_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTDlvIaZk8JT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        },
        "outputId": "64ce7651-d6e9-496c-8d51-642aacf61c89"
      },
      "source": [
        "print(\"------------------- NMF_conclusion -----------------------\")\n",
        "get_nmf_topics(model, 30)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------- NMF_conclusion -----------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic # 01</th>\n",
              "      <th>Topic # 02</th>\n",
              "      <th>Topic # 03</th>\n",
              "      <th>Topic # 04</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>intervention</td>\n",
              "      <td>exercise</td>\n",
              "      <td>acupuncture</td>\n",
              "      <td>dietary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>modest</td>\n",
              "      <td>physical</td>\n",
              "      <td>qol</td>\n",
              "      <td>supplementation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>care</td>\n",
              "      <td>fatigue</td>\n",
              "      <td>life</td>\n",
              "      <td>vitamin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>based</td>\n",
              "      <td>train</td>\n",
              "      <td>symptoms</td>\n",
              "      <td>reduce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>health</td>\n",
              "      <td>activity</td>\n",
              "      <td>reduce</td>\n",
              "      <td>prostate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>psychological</td>\n",
              "      <td>aerobic</td>\n",
              "      <td>adverse</td>\n",
              "      <td>term</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>feasible</td>\n",
              "      <td>survivors</td>\n",
              "      <td>advance</td>\n",
              "      <td>fat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>self</td>\n",
              "      <td>life</td>\n",
              "      <td>relieve</td>\n",
              "      <td>diet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>nurse</td>\n",
              "      <td>intensity</td>\n",
              "      <td>combine</td>\n",
              "      <td>long</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>outcomes</td>\n",
              "      <td>moderate</td>\n",
              "      <td>important</td>\n",
              "      <td>intake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>cognitive</td>\n",
              "      <td>week</td>\n",
              "      <td>tcm</td>\n",
              "      <td>decision</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>future</td>\n",
              "      <td>supervise</td>\n",
              "      <td>depression</td>\n",
              "      <td>weight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>psychosocial</td>\n",
              "      <td>fitness</td>\n",
              "      <td>enhance</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>distress</td>\n",
              "      <td>rehabilitation</td>\n",
              "      <td>fatigue</td>\n",
              "      <td>colorectal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>education</td>\n",
              "      <td>body</td>\n",
              "      <td>vomit</td>\n",
              "      <td>benefit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>brief</td>\n",
              "      <td>based</td>\n",
              "      <td>nausea</td>\n",
              "      <td>loss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>educational</td>\n",
              "      <td>related</td>\n",
              "      <td>liver</td>\n",
              "      <td>beta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>term</td>\n",
              "      <td>functional</td>\n",
              "      <td>manage</td>\n",
              "      <td>carotene</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>physical</td>\n",
              "      <td>capacity</td>\n",
              "      <td>hot</td>\n",
              "      <td>pattern</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>survivors</td>\n",
              "      <td>benefit</td>\n",
              "      <td>anxiety</td>\n",
              "      <td>appear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>home</td>\n",
              "      <td>qol</td>\n",
              "      <td>better</td>\n",
              "      <td>follow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>tailor</td>\n",
              "      <td>reduce</td>\n",
              "      <td>worthy</td>\n",
              "      <td>postmenopausal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>maintain</td>\n",
              "      <td>home</td>\n",
              "      <td>related</td>\n",
              "      <td>up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>pa</td>\n",
              "      <td>strength</td>\n",
              "      <td>nurse</td>\n",
              "      <td>years</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>crc</td>\n",
              "      <td>colon</td>\n",
              "      <td>care</td>\n",
              "      <td>serum</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>activity</td>\n",
              "      <td>cancer</td>\n",
              "      <td>reflexology</td>\n",
              "      <td>indicate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>acceptable</td>\n",
              "      <td>postmenopausal</td>\n",
              "      <td>palliative</td>\n",
              "      <td>health</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>approach</td>\n",
              "      <td>muscle</td>\n",
              "      <td>psychological</td>\n",
              "      <td>dose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>family</td>\n",
              "      <td>safe</td>\n",
              "      <td>flash</td>\n",
              "      <td>tocopherol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>larger</td>\n",
              "      <td>cognitive</td>\n",
              "      <td>psychotherapy</td>\n",
              "      <td>short</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Topic # 01      Topic # 02     Topic # 03       Topic # 04\n",
              "0    intervention        exercise    acupuncture          dietary\n",
              "1          modest        physical            qol  supplementation\n",
              "2            care         fatigue           life          vitamin\n",
              "3           based           train       symptoms           reduce\n",
              "4          health        activity         reduce         prostate\n",
              "5   psychological         aerobic        adverse             term\n",
              "6        feasible       survivors        advance              fat\n",
              "7            self            life        relieve             diet\n",
              "8           nurse       intensity        combine             long\n",
              "9        outcomes        moderate      important           intake\n",
              "10      cognitive            week            tcm         decision\n",
              "11         future       supervise     depression           weight\n",
              "12   psychosocial         fitness        enhance              low\n",
              "13       distress  rehabilitation        fatigue       colorectal\n",
              "14      education            body          vomit          benefit\n",
              "15          brief           based         nausea             loss\n",
              "16    educational         related          liver             beta\n",
              "17           term      functional         manage         carotene\n",
              "18       physical        capacity            hot          pattern\n",
              "19      survivors         benefit        anxiety           appear\n",
              "20           home             qol         better           follow\n",
              "21         tailor          reduce         worthy   postmenopausal\n",
              "22       maintain            home        related               up\n",
              "23             pa        strength          nurse            years\n",
              "24            crc           colon           care            serum\n",
              "25       activity          cancer    reflexology         indicate\n",
              "26     acceptable  postmenopausal     palliative           health\n",
              "27       approach          muscle  psychological             dose\n",
              "28         family            safe          flash       tocopherol\n",
              "29         larger       cognitive  psychotherapy            short"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}